{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/cancer-classification/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# 1. 데이터 로드\n",
    "train = pd.read_csv('dataset/train.csv', index_col='ID')\n",
    "test = pd.read_csv('dataset/test.csv', index_col='ID')\n",
    "\n",
    "# 2. 타겟 값 'SUBCLASS' 분리\n",
    "y = train['SUBCLASS']\n",
    "X = train.drop(columns=['SUBCLASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리했던 것들을 불러오는 부분\n",
    "X_encoded_pca=joblib.load('./joblib/X_encoded_pca.csv')\n",
    "X_encoded_pca_df = pd.DataFrame(X_encoded_pca)\n",
    "y_df = pd.DataFrame(y)\n",
    "y_df.reset_index(drop=True, inplace=True)\n",
    "test_encoded_pca=joblib.load('./joblib/test_encoded_pca.csv')\n",
    "test_encoded_pca_df = pd.DataFrame(test_encoded_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonModels\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.8.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #169-Ubuntu SMP Tue Jun 6 22:23:09 UTC 2023\n",
      "CPU Count:          8\n",
      "Memory Avail:       463.23 GB / 503.56 GB (92.0%)\n",
      "Disk Space Avail:   1798.24 GB / 1862.65 GB (96.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "2024-09-10 05:11:14,172\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-09-10 05:11:17,010\tWARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1073676288 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-09-10 05:11:18,202\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"AutoGluonModels/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Beginning AutoGluon training ... Time limit = 894s\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m AutoGluon will save models to \"AutoGluonModels/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Train Data Rows:    5512\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Train Data Columns: 3168\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Label Column:       SUBCLASS\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Problem Type:       multiclass\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Train Data Class Count: 26\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tAvailable Memory:                    473609.79 MB\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tTrain Data (Original)  Memory Usage: 133.22 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t\t('float', []) : 3168 | ['0', '1', '2', '3', '4', ...]\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t\t('float', []) : 3168 | ['0', '1', '2', '3', '4', ...]\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t39.0s = Fit runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t3168 features in original data used to generate 3168 features in processed data.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tTrain Data (Processed) Memory Usage: 133.22 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Data preprocessing and feature engineering runtime = 39.69s ...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting 110 L1 models ...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 569.42s of the 854.24s of remaining time.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.1693\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t1.68s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t7.5s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 557.39s of the 842.21s of remaining time.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.1435\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t1.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t7.33s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 545.73s of the 830.55s of remaining time.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.31%)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.2257\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t146.48s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.57s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 393.25s of the 678.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.47%)\n",
      "\u001b[36m(_ray_fit pid=349230)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=349281)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=349332)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=349384)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=349435)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=349485)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=349538)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=349595)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.2511\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t392.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.27s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 269.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.692, 'NeuralNetFastAI_BAG_L1': 0.308}\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.268\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.33s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting 108 L2 models ...\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 268.9s of the 268.18s of remaining time.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.31%)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.3777\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t160.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.67s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 101.91s of the 101.17s of remaining time.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.48%)\n",
      "\u001b[36m(_ray_fit pid=350616)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=350667)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=350718)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=350769)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=350821)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=350872)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=350923)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=350974)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.2638\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t163.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.34s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -122.18s of remaining time.\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.5, 'NeuralNetFastAI_BAG_L2': 0.292, 'LightGBMXT_BAG_L2': 0.167, 'NeuralNetFastAI_BAG_L1': 0.042}\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.3911\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m AutoGluon training complete, total runtime = 1017.16s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 185.7 rows/s (689 batch size)\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonModels/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=347788)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L3       0.386067   0.391147    accuracy        7.545541      16.684177  866.050268                 0.005137                0.002410           0.448966            3       True          8\n",
      "1  NeuralNetFastAI_BAG_L2       0.365747   0.377721    accuracy        7.156292      16.339576  702.462067                 1.401521                0.671685         160.100751            2       True          6\n",
      "2     WeightedEnsemble_L2       0.252540   0.267961    accuracy        3.431050       0.844687  539.349320                 0.004094                0.002414           0.325503            2       True          5\n",
      "3       LightGBMXT_BAG_L2       0.246734   0.263788    accuracy        6.138883      16.010082  705.500552                 0.384112                0.342191         163.139235            2       True          7\n",
      "4       LightGBMXT_BAG_L1       0.223512   0.251089    accuracy        0.331309       0.273308  392.541340                 0.331309                0.273308         392.541340            1       True          4\n",
      "5  NeuralNetFastAI_BAG_L1       0.219158   0.225689    accuracy        3.095648       0.568965  146.482477                 3.095648                0.568965         146.482477            1       True          3\n",
      "6   KNeighborsUnif_BAG_L1       0.172714   0.169267    accuracy        1.136015       7.500086    1.678666                 1.136015                7.500086           1.678666            1       True          1\n",
      "7   KNeighborsDist_BAG_L1       0.140784   0.143505    accuracy        1.191800       7.325532    1.658833                 1.191800                7.325532           1.658833            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1034s\t = DyStack   runtime |\t2566s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2566s\n",
      "AutoGluon will save models to \"AutoGluonModels\"\n",
      "Train Data Rows:    6201\n",
      "Train Data Columns: 3168\n",
      "Label Column:       SUBCLASS\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 26\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    472473.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 149.88 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3168 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3168 | ['0', '1', '2', '3', '4', ...]\n",
      "\t39.2s = Fit runtime\n",
      "\t3168 features in original data used to generate 3168 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 149.88 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 40.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1683.42s of the 2525.66s of remaining time.\n",
      "\t0.1664\t = Validation score   (accuracy)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t9.33s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1669.71s of the 2511.94s of remaining time.\n",
      "\t0.1368\t = Validation score   (accuracy)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t9.21s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1656.12s of the 2498.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.35%)\n",
      "\t0.228\t = Validation score   (accuracy)\n",
      "\t167.33s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1482.73s of the 2324.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.52%)\n",
      "\t0.2908\t = Validation score   (accuracy)\n",
      "\t1296.73s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 179.33s of the 1021.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.52%)\n",
      "\t0.1816\t = Validation score   (accuracy)\n",
      "\t215.2s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 789.73s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.846, 'NeuralNetFastAI_BAG_L1': 0.154}\n",
      "\t0.3019\t = Validation score   (accuracy)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 789.23s of the 788.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.36%)\n",
      "\t0.4003\t = Validation score   (accuracy)\n",
      "\t180.07s\t = Training   runtime\n",
      "\t0.76s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 602.35s of the 601.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.53%)\n",
      "\t0.4103\t = Validation score   (accuracy)\n",
      "\t582.87s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 12.94s of the 12.3s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.53%)\n",
      "\t0.3403\t = Validation score   (accuracy)\n",
      "\t209.44s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -260.28s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.429, 'LightGBMXT_BAG_L2': 0.429, 'NeuralNetFastAI_BAG_L2': 0.143}\n",
      "\t0.4414\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2827.24s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 156.9 rows/s (776 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonModels\")\n"
     ]
    }
   ],
   "source": [
    "# 3. AutoML을 사용한 학습 및 예측\n",
    "# AutoGluon은 범주형 인코딩을 자동으로 처리해줍니다.\n",
    "predictor = TabularPredictor(label='SUBCLASS', problem_type='multiclass', path='AutoGluonModels').fit(\n",
    "    train_data=pd.concat([X_encoded_pca_df, y_df], axis=1), \n",
    "    presets='best_quality', \n",
    "    ag_args_fit={'num_gpus': 1}  # GPU 사용 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 테스트 데이터에 대해 예측 수행\n",
    "predictions = predictor.predict(test_encoded_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./joblib/automl_model.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 모델 저장\n",
    "joblib.dump(predictor, './joblib/automl_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    model  score_val eval_metric  pred_time_val     fit_time  \\\n",
      "0     WeightedEnsemble_L3   0.441380    accuracy      21.168896  2446.497848   \n",
      "1       LightGBMXT_BAG_L2   0.410256    accuracy      20.405257  2265.798948   \n",
      "2  NeuralNetFastAI_BAG_L2   0.400258    accuracy      20.721702  1862.998975   \n",
      "3         LightGBM_BAG_L2   0.340268    accuracy      20.347774  1892.362649   \n",
      "4     WeightedEnsemble_L2   0.301887    accuracy       1.154048  1464.495476   \n",
      "5       LightGBMXT_BAG_L1   0.290760    accuracy       0.454375  1296.732733   \n",
      "6  NeuralNetFastAI_BAG_L1   0.228028    accuracy       0.697115   167.332039   \n",
      "7         LightGBM_BAG_L1   0.181584    accuracy       0.267574   215.200279   \n",
      "8   KNeighborsUnif_BAG_L1   0.166425    accuracy       9.334132     1.833763   \n",
      "9   KNeighborsDist_BAG_L1   0.136752    accuracy       9.207412     1.827935   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.002545           0.626674            3       True   \n",
      "1                0.444649         582.872199            2       True   \n",
      "2                0.761095         180.072225            2       True   \n",
      "3                0.387167         209.435899            2       True   \n",
      "4                0.002559           0.430704            2       True   \n",
      "5                0.454375        1296.732733            1       True   \n",
      "6                0.697115         167.332039            1       True   \n",
      "7                0.267574         215.200279            1       True   \n",
      "8                9.334132           1.833763            1       True   \n",
      "9                9.207412           1.827935            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0         10  \n",
      "1          8  \n",
      "2          7  \n",
      "3          9  \n",
      "4          6  \n",
      "5          4  \n",
      "6          3  \n",
      "7          5  \n",
      "8          1  \n",
      "9          2  \n"
     ]
    }
   ],
   "source": [
    "# 5. 결과를 submission.csv로 저장\n",
    "submission = pd.DataFrame({'ID': test.index, 'SUBCLASS': predictions})\n",
    "submission.to_csv('automl_submission.csv', index=False)\n",
    "\n",
    "# 6. 모델 성능 평가 (필요시)\n",
    "leaderboard = predictor.leaderboard(silent=True)\n",
    "print(leaderboard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
